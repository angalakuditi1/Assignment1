{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c4679b-1cc3-4065-aca0-4ea8b69cb55d",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11ad43eb-0586-4d3d-8e20-1c2ef52eea59",
   "metadata": {},
   "source": [
    "Hierarchical clustering is a way to group similar data points step by step, like organizing items into a family tree. Imagine you’re grouping friends based on shared hobbies. First, each person stands alone. Then, the two most similar people are grouped. This process continues, combining the closest groups, until everyone is connected in one big tree. You can “cut” this tree at any level to form the number of groups you want.\n",
    "\n",
    "What makes it different from other methods, like K-Means, is that you don’t need to say how many groups you want at the start. Also, hierarchical clustering builds a visual tree (called a dendrogram), which helps you see how data points are related at different levels. It’s great for exploring data and finding natural groupings, especially when you’re unsure how many clusters to expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf49ff1-2057-42f0-b666-ea3f490c1169",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief."
   ]
  },
  {
   "cell_type": "raw",
   "id": "805aaf3a-125f-4a24-98af-70143fbf4886",
   "metadata": {},
   "source": [
    "Agglomerative Clustering (Bottom-Up):\n",
    "This method starts with each data point as its own separate cluster. Then, step by step, it merges the two closest clusters based on a distance measure (like how similar or near they are) until all points are in one big cluster. It's like joining small friend groups into bigger ones until everyone is in one large group.\n",
    "\n",
    "Divisive Clustering (Top-Down):\n",
    "This starts the opposite way — with all data points in one big cluster. Then, it keeps splitting the cluster into smaller ones based on differences between points. It continues until each data point stands on its own or until a desired number of clusters is reached. Think of it like breaking a large group of students into smaller and smaller teams based on their interests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd429f-44f4-49fc-ac23-aacc8d71bda4",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
    "common distance metrics used?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7746125e-eb4a-44d0-adf9-314652efa5be",
   "metadata": {},
   "source": [
    "In hierarchical clustering, the distance between two clusters helps decide which clusters to merge next. Think of it like grouping similar friends in a party—how close two groups are tells you if they should be combined. To measure this closeness, we use linkage methods:\n",
    "\n",
    "Single Linkage: Measures the shortest distance between any two points from different clusters.\n",
    "Complete Linkage: Uses the farthest distance between any two points.\n",
    "Average Linkage: Calculates the average distance between all pairs of points from both clusters.\n",
    "Centroid Linkage: Uses the distance between the centers (average position) of the clusters.\n",
    "\n",
    "These linkages rely on distance metrics like:\n",
    "\n",
    "Euclidean distance (straight line),\n",
    "\n",
    "Manhattan distance (like walking city blocks),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8862f5-ee9a-464b-b950-734f79207846",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
    "common methods used for this purpose?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6e0b6ca-c1c0-421b-8335-ea6d58acf85f",
   "metadata": {},
   "source": [
    "To find the optimal number of clusters in hierarchical clustering, we need to decide where to “cut” the dendrogram (tree diagram). This cut determines how many groups (clusters) we divide the data into. One simple method is to visually inspect the dendrogram and look for the biggest vertical distance between merges — this gap suggests a natural place to split the data.\n",
    "\n",
    "Another method is the elbow method, where we plot the number of clusters against the total distance (or error) within each cluster. The point where the curve starts to flatten, like an elbow, is often the best number of clusters.\n",
    "\n",
    "There's also the silhouette score, which measures how similar data points are within their own cluster compared to others — a higher score means better clustering.\n",
    "\n",
    "These methods help us avoid too few or too many clusters, making the grouping more meaningful and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ff40a-74cb-4b01-836b-801584150b55",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d936fec-450a-4a17-a20a-4a5fbac94413",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-like diagram used in hierarchical clustering to show how data points are grouped step by step. Imagine each item as a leaf on a tree. As similar items are found, they are joined together, forming branches. This process continues until all items are connected under one big tree. Dendrograms help us visually understand which data points are most alike and when they were grouped during the clustering process. By cutting the tree at a certain height, we can decide how many clusters to form. It’s a simple way to see patterns and relationships in complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17752f61-d127-470f-b930-dfca765d1263",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
    "distance metrics different for each type of data?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "feac4a2e-327b-43b2-bf34-30f585ef0e30",
   "metadata": {},
   "source": [
    "Yes, hierarchical clustering can be used for both numerical and categorical data, but the way we measure \"distance\" between data points is different.\n",
    "\n",
    "For numerical data, distance is usually measured using Euclidean distance—like drawing a straight line between two points on a graph. This works well when values are continuous, such as height or weight.\n",
    "\n",
    "For categorical data, we can’t use Euclidean distance. Instead, we use similarity-based measures like:\n",
    "\n",
    "Hamming distance: counts how many attributes are different between two items.\n",
    "\n",
    "Jaccard similarity: measures how similar sets are, especially useful for binary or yes/no type data.\n",
    "\n",
    "If the dataset has mixed types (both numerical and categorical), techniques like Gower’s distance are used, which combine different distance measures based on the type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2049b-3c6c-4e07-8b26-53a7d3b347df",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1621351-b957-40cc-99d9-6fe85916462c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
