{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33498010-da3a-419b-9e49-e212eeca1fa7",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a68c2d4-d701-458f-a375-ed61c6a14216",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to normalize numerical features within a specific range, typically between 0 and 1. It ensures that all features have the same scale, preventing dominant features from overshadowing others in machine learning models. This technique is especially useful for algorithms that rely on distance calculations, such as k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Example:\n",
    "Consider a dataset where age values range from 20 to 60. If we apply Min-Max scaling to transform these values into a range of 0 to 1, the smallest value (20) becomes 0, the largest value (60) becomes 1, and other values are proportionally adjusted. For instance, an age of 30 would be mapped closer to 0, while an age of 50 would be mapped closer to 1.\n",
    "\n",
    "This transformation helps models converge faster and improves their performance by ensuring all features contribute equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742576c-a858-49f0-8d0f-2b5cb71033d6",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "57fb4185-c040-499b-a6cd-d6f0ef434f23",
   "metadata": {},
   "source": [
    "he Unit Vector technique (also called Normalization) scales data so that its total length (magnitude) becomes 1 while keeping its direction the same. Imagine a cricket score of a batsman represented as a two-dimensional vector [3,4], meaning 3 runs in one format and 4 in another. Normalization scales this to [0.6,0.8], maintaining the ratio but adjusting the size.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "Min-Max scaling squeezes data into a fixed range (e.g., 0 to 1), while Unit Vector scaling keeps the proportions but ensures the total value sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af772b27-d0f3-4748-96f8-87cefd68550f",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a167da95-332e-4c13-b2c4-085507719fca",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) â€“ Simple Explanation\n",
    "Imagine we have a huge dataset with many columns (features), like a survey with 100 questions. Analyzing all 100 questions is difficult, but what if we could summarize the most important information using just a few new \"super questions\" Thatâ€™s what PCA doesâ€”it reduces the number of features while keeping the essential patterns in the data.\n",
    "\n",
    "How PCA Works for Dimensionality Reduction\n",
    "Find the Important Patterns â€“ PCA identifies the directions (called principal components) where data varies the most.\n",
    "\n",
    "Transform the Data â€“ It reorients the dataset along these new directions.\n",
    "\n",
    "Drop Less Important Information â€“ We keep only the most significant components and discard the rest, reducing complexity.\n",
    "\n",
    "Imagine a dataset of handwritten numbers (0â€“9) with each image having 784 features (28x28 pixels). Instead of analyzing all 784 features, PCA can reduce it to, say, 50 features while keeping most of the important details. This makes machine learning models run faster and avoids unnecessary noise.\n",
    "\n",
    "In short, PCA is like summarizing a long novel into a short, meaningful summary while keeping the core story intact! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf2057-40f1-489f-a54c-e136c1394b15",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d934efb-046e-4c46-818f-85e89fa71978",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) helps simplify complex data by finding patterns and reducing the number of features while keeping the most important information. It doesnâ€™t just pick some features and remove othersâ€”it creates new features (called principal components) that are combinations of the original ones.\n",
    "\n",
    "How PCA helps in Feature Extraction\n",
    "Imagine you have a dataset with many columns (features), but not all of them are equally important. PCA finds the most meaningful patterns in the data and creates new features that capture the maximum variation. This helps in reducing redundancy and making the data easier to analyze.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset of student performance with features like homework scores, exam scores, and class participation. PCA can combine these into a smaller set of features representing overall performance, making it easier to analyze and use for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49b6a0-5dbd-4351-873b-4aba14166b02",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4942220f-f869-4a6e-b572-84e536fa8bb0",
   "metadata": {},
   "source": [
    "see, we have given like price and rating and delivery time like that so, the minmax scalar does is it will reduce the rows content into equal value trhought the values like for example lets say the price can be - 1000,300,4490 something like that and rating can be 1 to 5 and delivery time in hrs or minutes so, it takes the data in that form and convert them in range of values it can be 0 to 1 aor -1 to 1 something like that here im using one example to show how it's done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "106d7eb4-808c-425a-8148-22b2a73d8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      price    rating  delivery_time\n",
      "0  0.000000  0.000000       0.000000\n",
      "1  0.411765  0.500000       0.333333\n",
      "2  0.823529  1.000000       0.666667\n",
      "3  1.000000  0.666667       1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'price': [150, 500, 850, 1000],  # in â‚¹\n",
    "    'rating': [2, 3.5, 5, 4],  # out of 5\n",
    "    'delivery_time': [15, 30, 45, 60]  # in minutes\n",
    "})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scale = scaler.fit_transform(data)\n",
    "scpd = pd.DataFrame(scale,columns=data.columns)\n",
    "print(scpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd4bdf-082f-44ff-a18f-f50f506cf7de",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a6ea21e-19d3-45f1-a3ec-587be8e74ca8",
   "metadata": {},
   "source": [
    "PCA reduces dimensionality in stock price prediction by transforming correlated features into independent principal components while preserving variance. First, the dataset is standardized using StandardScaler to ensure equal feature contribution. Next, PCA(n_components=0.95) is applied to retain 95% of the variance. The explained variance plot helps determine the optimal number of components. The transformed dataset is then used to train models like Linear Regression, Random Forest, or LSTM. PCA improves efficiency, reduces overfitting, and handles multicollinearity. However, it may not be suitable if interpretability is crucial or if deep learning models require raw feature relationships for better predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc0e4477-daae-48c3-bf69-087c0a7c38ba",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9956c6c-19a4-4af8-9f32-55816b123a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data=[1, 5, 10, 15, 20]\n",
    "data= np.array([1, 5, 10, 15, 20]).reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scalern = scaler.fit_transform(data)\n",
    "scalern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a6a36-ecb6-4dde-ae8b-ca5e10f25911",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a1bdf54-9e0d-4204-9c95-77463233c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of principal components: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'height': [170, 160, 180, 175, 165], \n",
    "    'weight': [70, 60, 80, 75, 65], \n",
    "    'age': [25, 30, 35, 40, 45], \n",
    "    'gender': [1, 0, 1, 0, 1],  # 1 = Male, 0 = Female\n",
    "    'blood_pressure': [120, 110, 130, 125, 115]\n",
    "})\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Step 4: Find number of components that explain at least 95% variance\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Optimal number of principal components: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11860dfb-f028-40a3-91c9-e7acd7b06b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
