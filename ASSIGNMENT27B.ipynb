{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecef69a0-da06-4606-b524-05414cc6c99d",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebc3adf7-d5f4-4578-9629-1e039279a3fe",
   "metadata": {},
   "source": [
    "n K-Nearest Neighbors (KNN), Euclidean and Manhattan distances are two ways to measure how far apart two points are in a space.\n",
    "\n",
    "Euclidean distance is like measuring the straight-line distance between two points, similar to how you would use a ruler to find the shortest path between two locations on a map. It's calculated using the Pythagorean theorem:\n",
    "This gives the shortest distance between the points.\n",
    "\n",
    "Manhattan distance, on the other hand, is like walking along the streets in a grid of blocks (think of the layout of a city like Manhattan, where you have to move along the streets rather than cutting across directly). It's calculated as:\n",
    "This gives the distance if you can only move in straight lines along the axes, not diagonally.\n",
    "\n",
    "Impact on KNN performance:\n",
    "Euclidean distance works well when the data points are spaced in a way where direct diagonal connections are meaningful, especially in continuous, smooth spaces.\n",
    "\n",
    "Manhattan distance is better for situations where movement is constrained to grid-like structures, such as in urban planning or if the features are on different scales.\n",
    "\n",
    "In KNN, the distance metric affects how the algorithm defines \"closeness,\" which can influence which points are considered neighbors, impacting both classification and regression results. The wrong choice can lead to misclassifications or inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd68dc2-1e1f-41a3-a888-31d78ac0af6b",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "900d734a-cb61-4d05-9634-4427e273d925",
   "metadata": {},
   "source": [
    "To choose the optimal value of k for a KNN (K-Nearest Neighbors) classifier or regressor, you need to find a value that balances underfitting and overfitting. A small k makes the model sensitive to noise and may lead to overfitting, while a large k might smooth out the decision boundaries, causing underfitting. The key is finding a k that generalizes well to unseen data.\n",
    "\n",
    "One common technique to determine the best k is cross-validation. This involves dividing the data into multiple subsets (folds), training the model on some of them, and testing it on the remaining ones. You repeat this process for each possible combination and evaluate how well the model performs.\n",
    "\n",
    "Another method is using a grid search, where you systematically test a range of k values and pick the one that gives the best performance based on metrics like accuracy, precision, or mean squared error (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38560dab-c92a-4415-bfda-087af1fb9c12",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "618e09c4-2765-4dd1-bcff-d23c2a6b4255",
   "metadata": {},
   "source": [
    "In a KNN (K-Nearest Neighbors) classifier or regressor, the choice of distance metric—like Euclidean, Manhattan, or Minkowski—determines how the algorithm measures the \"closeness\" between data points. The distance metric affects how neighbors are identified, and thus, how predictions are made.\n",
    "\n",
    "For example, Euclidean distance measures straight-line distance, making it useful when data points are on a similar scale and in a continuous space. If the data has lots of outliers, Manhattan distance (which adds up absolute differences) might be better as it's less sensitive to extreme values.\n",
    "\n",
    "Choosing the right metric depends on the problem. Euclidean is ideal for well-scaled data with similar features, while Manhattan may work better when dealing with high-dimensional or sparse data. Minkowski, a generalization of both, gives flexibility to experiment based on the situation. The metric you choose can affect accuracy, so it's important to test different ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09e3b3-ea32-4b36-b9cd-c09fef4f9ce2",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce376362-1699-42a3-bee8-4dbd95f541b4",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors (KNN), a few key hyperparameters control how the model works. Think of KNN like finding the closest friends to help make a decision — the \"closest\" is based on distance.\n",
    "\n",
    "n_neighbors (K value): This tells the model how many neighbors to look at when making a prediction. A small value (like 1) may lead to noisy, overfitted results, while a large value may smooth things too much and miss details.\n",
    "\n",
    "weights: It decides how to value the neighbors — all equal or closer ones given more importance. \"uniform\" gives equal weight, \"distance\" gives more importance to nearer neighbors.\n",
    "\n",
    "metric: It defines how distance is measured. Common ones include \"euclidean\" (straight-line distance) and \"manhattan\" (like walking city blocks). The right one depends on your data type and shape.\n",
    "\n",
    "To tune these hyperparameters, you can use Grid Search or Random Search with cross-validation. This means trying different combinations and checking which gives the best accuracy on unseen data. Tools like GridSearchCV in scikit-learn help automate this.\n",
    "\n",
    "In short, tuning KNN is like choosing how many and which friends to ask for advice — and weighing their opinions based on how close they are to your situation. Choosing wisely helps the model give better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541746df-00c6-4867-a3ea-08a51c2742fa",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20e21728-0aa7-412c-92fe-1e714d808973",
   "metadata": {},
   "source": [
    "In simple terms, the size of the training set greatly affects how well a KNN (K-Nearest Neighbors) model performs. If the training set is too small, the model won’t have enough examples to compare with, leading to poor accuracy. It might make wrong predictions because it doesn't “know” enough. On the other hand, a very large training set gives better results, but it also makes the model slower because it needs to compare with more data points during prediction.\n",
    "\n",
    "To optimize training set size, techniques like cross-validation can be used to find the right balance between accuracy and speed. Feature selection and dimensionality reduction (like PCA) help by focusing only on the most important information, reducing unnecessary data. Also, sampling methods like stratified sampling can ensure that the training set is well-balanced and representative of the overall data, improving the KNN model's performance without needing too much data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cbd9f-a36c-44f3-a2c0-179092ff2076",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a62f8b3-0538-4327-862e-318f450c015f",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is simple but has some downsides. It gets slow with large data since it checks every point to make predictions. It also struggles when data isn’t clean or has too many features (called \"curse of dimensionality\"). It’s sensitive to irrelevant features and noisy data, which can lead to poor results. To improve it, we can scale the data, remove unnecessary features, and use fewer but more relevant ones. Also, choosing a good value for \"K\" using cross-validation helps. In big datasets, switching to faster models like decision trees or SVMs can be better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
