{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb354f18-a472-4e93-a52f-dd69b75fa3a0",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78dee8a6-cd1e-4f8b-b98c-e6a0e66a63df",
   "metadata": {},
   "source": [
    "R-squared (R²) is a statistical measure that tells us how well a linear regression model fits the data. It shows how much of the variation in the dependent variable (output) can be explained by the independent variable(s) (input).\n",
    "\n",
    "Think of it like predicting cricket scores based on past performances. If a model predicts scores accurately most of the time, it has a high R². If the predictions are way off, the R² is low. It ranges from 0 to 1, where 0 means the model is useless, and 1 means a perfect prediction (which rarely happens in real life).\n",
    "\n",
    "A high R² suggests the model is doing a good job, but an extremely high value might mean it's too closely following the training data (overfitting). It’s a helpful metric, but it should always be checked along with other factors like residual errors to ensure a reliable and generalizable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfce69-adf5-4f92-a240-b83df3afd6f5",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0547359e-bddf-45a7-be7e-c58a8c099b8b",
   "metadata": {},
   "source": [
    "Adjusted R-squared is an improved version of R-squared, which tells us how well a regression model explains the variation in the data. The regular R-squared increases whenever more variables (independent variables) are added to the model, even if they don’t actually improve the model’s accuracy. This can be misleading.\n",
    "\n",
    "Adjusted R-squared fixes this issue by penalizing unnecessary variables. If a new variable genuinely improves the model, the Adjusted R-squared will increase. But if the variable is irrelevant, it will decrease.\n",
    "\n",
    "In short, Adjusted R-squared is a refined measure that prevents overfitting and gives a realistic evaluation of a regression model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa892d2-735e-49c0-a70a-e9cd6a30bc03",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7bbd5a3-cdfd-46cc-9b77-7114bec2f18c",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate when you have multiple independent variables in your regression model. Regular R-squared measures how well the model fits the data, but it has a flaw: it always increases when you add more variables, even if they don’t truly improve the model. This can be misleading.\n",
    "\n",
    "Adjusted R-squared fixes this issue by considering the number of variables and only increasing if the new variable actually improves the model’s accuracy. If a variable doesn’t help, adjusted R-squared can even decrease.\n",
    "\n",
    "Use adjusted R-squared when comparing models with different numbers of variables, as it helps avoid overfitting and ensures you're only keeping valuable predictors. It gives a more realistic measure of how well your model explains the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5a163-41dc-431b-8f23-8e65fbe40e64",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba83b896-a3e3-459b-bf81-435d764d9a92",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are used to measure how well a regression model predicts values compared to actual values.\n",
    "\n",
    "MAE (Mean Absolute Error) tells us the average amount by which predictions differ from actual values, treating all errors equally. It gives a simple idea of how much the model is off, on average.\n",
    "\n",
    "MSE (Mean Squared Error) is similar but gives more weight to larger errors by squaring the differences before averaging them. This means that big mistakes have a greater impact on the final score.\n",
    "\n",
    "RMSE (Root Mean Squared Error) is just the square root of MSE, making it easier to interpret since it has the same units as the target variable.\n",
    "\n",
    "Smaller values for these metrics indicate better model performance. MAE is straightforward, MSE penalizes big mistakes more, and RMSE helps in understanding errors in real-world terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea373264-87b9-4186-8df6-ac055c582223",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "49bd39ab-190b-42c5-acc8-a01766c2b1e8",
   "metadata": {},
   "source": [
    "Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis\n",
    "1. Mean Absolute Error (MAE)\n",
    "Advantages:\n",
    "\n",
    "Easy to understand since it measures the average absolute difference between predicted and actual values.\n",
    "Less sensitive to large errors (outliers).\n",
    "\n",
    "Disadvantages:\n",
    "Ignores the direction of errors \n",
    "Not ideal when large errors need to be penalized more.\n",
    "\n",
    "2. Mean Squared Error\n",
    "Advantages:\n",
    "\n",
    "Penalizes larger errors more due to squaring, making it useful when big mistakes should be avoided.\n",
    "Smooth for optimization algorithms.\n",
    "\n",
    "Disadvantages:\n",
    "Squaring makes the error scale different from the actual values, making it harder to interpret.\n",
    "Sensitive to outliers, as large errors dominate the metric.\n",
    "\n",
    "3. Root Mean Squared Error \n",
    "Advantages:\n",
    "Similar to MSE but in the same unit as the original data, making it more interpretable.\n",
    "Suitable when large errors need to be significantly penalized.\n",
    "\n",
    "Disadvantages:\n",
    "Still sensitive to outliers.\n",
    "Harder to compute than MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96aa18d-00d6-42ea-ae72-74cca6440012",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when isit more appropriate to use?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94c0ef12-f3d5-41b8-9a48-fd1d84977402",
   "metadata": {},
   "source": [
    "Lasso Regularization means Imagine we’re predicting house prices based on many features size, location, number of rooms, etc. Some features may be unnecessary, making the model too complex. Lasso regularization helps by forcing some features to have zero importance, removing them completely. This makes the model simpler and avoids overfitting (memorizing the data instead of learning patterns).\n",
    "\n",
    "Lasso vs. Ridge\n",
    "Lasso (L1 Regularization): Removes unnecessary features by making some coefficients exactly zero (like picking only the most important factors).\n",
    "\n",
    "Ridge (L2 Regularization): Reduces the impact of unnecessary features but doesn’t remove them completely. It shrinks all coefficients without setting any to zero.\n",
    "\n",
    "When to Use Lasso\n",
    "When you want a simpler model by removing unimportant features,When some features aren’t useful and should be eliminated,When you need automatic feature selection (Lasso picks the best ones for you)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0fe92-4d44-4ee5-8bef-5f657aaba47a",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5518a18-7a4d-4872-9715-e724c7fb818c",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting by adding a penalty (extra cost) to large coefficients in a linear model. Overfitting happens when a model learns too much from the training data, including noise, making it perform poorly on new data. Regularization ensures the model remains simple and generalizes well.\n",
    "\n",
    "There are two common types:\n",
    "\n",
    "Lasso (L1 Regularization): Shrinks some coefficients to zero, selecting only the most important features.\n",
    "\n",
    "Ridge (L2 Regularization): Shrinks all coefficients but keeps them small, reducing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26320903-7225-4699-8f05-5c4eceda3fe2",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff2c2a92-6b92-4b40-a302-58fa830289d8",
   "metadata": {},
   "source": [
    "Regularized linear models, like Ridge and Lasso regression, help prevent overfitting by adding penalties to large coefficients. However, they have limitations. If the data has complex, non-linear relationships, these models struggle because they assume a straight-line relationship. They also rely heavily on choosing the right penalty strength, which can be tricky. Lasso may shrink important variables to zero, removing useful information. Additionally, if there are many irrelevant variables, regularization won’t always pick the best ones. In short, while regularization helps control overfitting, it may not capture complex patterns, making other models like decision trees or neural networks better choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99c9ef-38d9-4a0c-af95-e6f72ba51e13",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "392c7d3e-015a-48da-8c11-b04589a19ca3",
   "metadata": {},
   "source": [
    "Since the two metrics belong to different models, direct comparison is not ideal.If we assume they are on the same dataset, Model B (MAE = 8) may be preferable as it has a lower average error.However, RMSE provides more insight into large deviations, and a complete evaluation should include other metrics like R² or MAPE before deciding which model is superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4d3c8-c836-4324-b7ad-0bedf5604230",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67fc5cb3-765a-4be3-bb90-c228ae381e50",
   "metadata": {},
   "source": [
    "Lasso is good for feature selection but may remove useful data.\n",
    "Ridge keeps all features but may not improve interpretability.\n",
    "If we need a balance, we can use Elastic Net, which combines both Ridge and Lasso.finally, If we care about simplicity and want fewer features, choose Lasso. If we want to keep all features and prevent overfitting, choose Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ecafeb-70ea-41a4-8ef4-9b1ef6dce9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
