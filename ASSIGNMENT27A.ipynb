{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9bf74b-2d94-4d95-9323-346c93c66435",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c657b8d-b4b7-46b9-b264-0db07d2f2035",
   "metadata": {},
   "source": [
    "KNN (K-Nearest Neighbors) is a simple machine learning algorithm used for classification and prediction. Imagine you move to a new area and want to guess which cricket team your neighbors support. You ask the closest 5 people (neighbors), and if 3 support Team A and 2 support Team B, you assume the area supports Team A. KNN works the same way — it looks at the \"K\" nearest data points to a new one and decides based on majority. It doesn't learn rules in advance; it just compares and chooses based on closeness (distance) to others. Simple, but powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b245f7-ae3d-4d76-8060-75b07b57d339",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0024b9e-3127-43ac-a154-720a985fd08c",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors (KNN), the value of K decides how many neighbors (data points) we look at to make a prediction. Choosing the right K is like finding the right balance between too specific and too general.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "If K is too small (like K=1), the model becomes very sensitive to noise and may give wrong predictions — like judging a movie based on just one person's opinion.\n",
    "\n",
    "If K is too large, it might include too many unrelated points — like asking 100 random people about a movie and getting mixed reviews that don’t help.\n",
    "\n",
    "So, we usually try different K values and use a method called cross-validation to check which one gives the best accuracy. Most often, we choose an odd number (to avoid ties) and a value that is neither too small nor too large. A good starting point is the square root of the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df80c2-e546-4107-8c29-011723410647",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a658aab-32cd-4228-a646-a768578f6293",
   "metadata": {},
   "source": [
    "In simple terms, KNN (K-Nearest Neighbors) is like finding the \"closest friends\" of a new person based on common features.\n",
    "\n",
    "KNN Classifier is used when we want to categorize something. For example, if you see a fruit and want to know whether it's an apple or orange, KNN classifier looks at the closest known fruits and picks the most common label among them.\n",
    "\n",
    "KNN Regressor is used when we want to predict a number. For example, if you want to predict someone's weight based on height and age, the KNN regressor checks the weights of similar people (neighbors) and returns the average.\n",
    "\n",
    "Main difference:\n",
    "\n",
    "Classifier → predicts a category or label (like Yes/No, Male/Female).\n",
    "\n",
    "Regressor → predicts a number or value (like price, age, temperature).\n",
    "\n",
    "Both work by checking the 'k' nearest neighbors, but what they return differs — category vs. number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b89659-f430-448f-826b-a9e4a4090267",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f20785dd-7c9d-4029-a93a-b30e75feefac",
   "metadata": {},
   "source": [
    "To measure the performance of K-Nearest Neighbors (KNN), we check how well it predicts the correct labels for data it hasn't seen before. In simple terms, we give KNN some test data and see how many times it guesses right versus wrong. Common ways to do this include accuracy (how many correct predictions out of total), precision (how many predicted positives are truly positive), and recall (how many actual positives were correctly found). We can also use a confusion matrix to see detailed results like true positives, false positives, etc. For more balanced evaluation, F1-score (a mix of precision and recall) is used. Sometimes, we use cross-validation—splitting data into parts to test multiple times—for reliable results. Lower error and higher scores mean KNN is doing a good job predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065eb1ab-7ded-4bee-a740-2c133bca0b7d",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77422c70-0fd0-49cb-879e-b013f520c33c",
   "metadata": {},
   "source": [
    "KNN works by finding the \"nearest neighbors\" using distance metrics (like Euclidean distance). When the number of features (dimensions) increases:\n",
    "\n",
    "All data points start to look equally distant from each other.It becomes hard to identify which points are actually nearest.The model performance drops because distance loses meaning in high dimensions.\n",
    "\n",
    "Example:\n",
    "Imagine a 2D space with points. You can easily see which point is closer. But in 100 dimensions,The space becomes sparse,The distance between any two points becomes almost the same.KNN can’t clearly decide who is a neighbor anymore.\n",
    "\n",
    "Why It’s a Problem in KNN:\n",
    "KNN is a lazy learner and relies heavily on distance.\n",
    "In high dimensions, distances become less reliable, leading to bad predictions.\n",
    "Also, computation time increases drastically with more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb847824-0755-43a5-8f3c-e23a76d12ff7",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe69bc75-374c-476a-a85f-53c609552377",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a distance-based algorithm that requires complete data for accurate computation. Missing values must be addressed before applying KNN, as they interfere with distance calculations. One common approach is imputation, where missing values are filled using statistical methods such as the mean, median, or mode of the respective feature. A more advanced technique is KNN imputation, where the missing value of a data point is estimated based on the values of its nearest neighbors, ensuring more context-aware filling. Alternatively, if the dataset has very few missing entries, rows with missing values can be removed. In cases where an entire column has excessive missing data and lacks significance, it may be dropped. After imputation, it is crucial to normalize or standardize the dataset to ensure fair distance measurement. Overall, proper handling of missing values is essential to maintain the reliability and accuracy of KNN predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cbcdf-9a72-4d6e-96b1-050899831e27",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39e4e57a-3808-4fc6-a96a-e12614792a14",
   "metadata": {},
   "source": [
    "In simple terms, K-Nearest Neighbors (KNN) can be used for both classification (predicting categories) and regression (predicting numbers).\n",
    "\n",
    "KNN Classifier is used when the output is a category (like spam or not spam, healthy or sick). It checks which categories are most common among the nearest data points and predicts based on majority vote.\n",
    "\n",
    "KNN Regressor is used when the output is a number (like price, temperature). It averages the values of nearby points to make a prediction.\n",
    "\n",
    "Performance-wise, KNN Classifier generally performs better on well-separated class problems, while KNN Regressor can struggle if the data has a lot of noise or outliers.\n",
    "\n",
    "Which is better?\n",
    "\n",
    "Use KNN Classifier for tasks like image recognition or disease prediction,Use KNN Regressor for tasks like predicting house prices or stock trends.Both are simple and effective, but not ideal for large datasets due to slow prediction speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839eb7c-60c6-4087-b5a8-5244433a6fd4",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7deca74f-b7e0-46b0-9103-16d3132c682e",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) – Strengths & Weaknesses (Layman Explanation, ~150 words)\n",
    "\n",
    "Strengths:\n",
    "KNN is simple and easy to understand. It works like asking your neighbors for advice—if most of them agree on something, you go with it. It’s great for small datasets and doesn’t need much training. KNN can work for both classification (like deciding if a fruit is an apple or orange) and regression (like predicting a price).\n",
    "\n",
    "Weaknesses:\n",
    "KNN becomes slow when dealing with large datasets because it has to check all data points each time. It’s also sensitive to irrelevant or differently scaled features—like comparing weight in grams and height in meters directly. It doesn’t perform well when data is noisy or high-dimensional (too many features).\n",
    "\n",
    "How to fix:\n",
    "Speed can be improved using algorithms like KD-Trees. Feature scaling (like normalization) helps make comparisons fair. Removing irrelevant features or using dimensionality reduction techniques like PCA can improve accuracy and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563283b6-82fb-491b-8c47-174e9f9341eb",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8462e003-6aac-485d-a853-0778c6a04eed",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors (KNN), distance helps us find how \"close\" two data points are. Two common ways to measure this are Euclidean distance and Manhattan distance.\n",
    "\n",
    "In layman's terms:\n",
    "\n",
    "Euclidean distance is like measuring the shortest straight line between two points—as if you were flying like a bird from one location to another.\n",
    "\n",
    "Manhattan distance is like walking on a grid of city streets—you can only move up/down or left/right, not diagonally.\n",
    "\n",
    "Imagine you're in a city like New York (which has a grid layout). To go from one building to another, you walk block by block (Manhattan distance). But if you could fly from rooftop to rooftop in a straight line, that would be Euclidean distance.\n",
    "\n",
    "In KNN, the choice between the two depends on the problem. Euclidean works best with continuous data, while Manhattan can perform better when features are high-dimensional or have many zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45ed5c-4d9a-4758-ba5c-7c00ec51ce34",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed89620c-eabd-4b5e-82e5-dfcb0027af97",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in the performance of K-Nearest Neighbors (KNN) because KNN is based on calculating the distance between data points. Here's why scaling is important:\n",
    "\n",
    "Distance Calculation Sensitivity: KNN uses distance metrics like Euclidean distance, Manhattan distance, or Minkowski distance to find the nearest neighbors of a data point. If one feature has a much larger scale than another (for example, one feature is in the range of 0–1, while another is in the range of 1–1000), the feature with the larger scale will dominate the distance calculation. This can lead to biased results where the model might incorrectly prioritize the feature with a larger range.\n",
    "\n",
    "Improved Accuracy: By scaling the features (for instance, using techniques like Min-Max scaling or Standardization), you ensure that all features contribute equally to the distance measure. This typically leads to better performance because the model can treat all features with equal importance.\n",
    "\n",
    "Faster Convergence: While KNN is a non-parametric model, scaling the features can help the algorithm work more efficiently, reducing computational overhead and improving performance during both training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fcff5-717b-404b-885f-b93ae8b4ed04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
