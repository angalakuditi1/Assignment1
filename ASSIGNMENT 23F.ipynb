{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370c3a98-9720-49e5-ab63-c095c0bb9473",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d920fde8-ee70-43b1-86ee-d15f85ecee05",
   "metadata": {},
   "source": [
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that helps in feature selection while preventing overfitting. Imagine you are trying to predict a cricket player’s score based on multiple factors like fitness, past performance, and weather conditions. Some factors may be more important than others, and some may not be useful at all.\n",
    "\n",
    "Lasso Regression automatically reduces the importance of less useful factors by shrinking their coefficients to zero, effectively removing them from the model. This makes it different from regular linear regression, which considers all features equally, and from Ridge Regression, which reduces coefficient values but does not remove them completely.\n",
    "\n",
    "The key advantage of Lasso Regression is that it simplifies the model by selecting only the most important variables, making predictions more accurate and interpretable, especially when dealing with large datasets with many features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c8955-422b-49fc-a2cb-140dab87d820",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e6cfb-65bc-4c9c-87b2-72a9860d0778",
   "metadata": {},
   "source": [
    "Lasso Regression is great for feature selection because it helps automatically remove unnecessary variables. Imagine you are analyzing cricket player performance, and you have many factors like runs, strike rate, height, and favorite food. Some of these factors don’t actually impact performance, but traditional regression keeps them all.\n",
    "\n",
    "Lasso adds a penalty to the model, shrinking the importance of less useful features to zero. This means it keeps only the most important ones, making the model simpler and more accurate. It's like packing for a trip—you only take essential things instead of carrying everything.\n",
    "\n",
    "The main advantage? Lasso makes models less complex, prevents overfitting, and improves accuracy by selecting only the most relevant features. This is super useful when dealing with large datasets with too many variables, helping data analysts focus on what truly matters while making computations faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414629f7-0087-486d-8c5b-12136b3df33f",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dee80d1-b2e9-43c8-8b85-774bf7e8e8a5",
   "metadata": {},
   "source": [
    "In Lasso Regression (Least Absolute Shrinkage and Selection Operator), the coefficients are interpreted in the following ways:\n",
    "\n",
    "Shrinkage of Coefficients:\n",
    "Unlike ordinary least squares (OLS) regression, where coefficients can take any value, Lasso regression applies L1 regularization, forcing some coefficients to shrink toward zero.\n",
    "\n",
    "This means that Lasso regression reduces the impact of less important features and can even set some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "Feature Selection and Sparsity:\n",
    "If a coefficient is exactly zero, it means that the corresponding feature is not important for the model and has been removed.\n",
    "This is useful in high-dimensional datasets where irrelevant features can introduce noise.\n",
    "\n",
    "Magnitude of Coefficients:\n",
    "The magnitude of a coefficient indicates the strength of the relationship between the independent variable and the target variable.\n",
    "A large absolute coefficient means that the variable is important, whereas a small or zero coefficient means it has little or no influence.\n",
    "\n",
    "Effect of λ (Regularization Parameter):\n",
    "The λ (lambda) parameter controls the amount of regularization:\n",
    "Small λ: Less regularization, coefficients similar to OLS regression.\n",
    "Large λ: Strong regularization, more coefficients shrink to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d59816-97bb-4260-b075-8d655c60b9af",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e63895-ac36-4fe3-b583-7857ab1864cc",
   "metadata": {},
   "source": [
    "In Lasso Regression, the key tuning parameter is alpha (λ), which controls the strength of penalty applied to the model.\n",
    "\n",
    "Alpha (λ): It determines how much the model shrinks the coefficients of less important features to zero.\n",
    "If α is too small, the model behaves like simple Linear Regression, keeping all features and risking overfitting.\n",
    "If α is too large, the model shrinks many coefficients to zero, making it too simple and possibly underfitting.\n",
    "\n",
    "By adjusting α, we balance the model’s complexity:\n",
    "A low α captures more details but may overfit.\n",
    "A high α simplifies the model by selecting only the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc1788-8edf-4da3-b853-dcb5fb349ea2",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed86f9bd-067e-4498-862f-c1e6b1da8658",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can be used for non-linear regression problems, but not directly. Lasso is a linear model that adds penalty terms to shrink coefficients, helping in feature selection. To handle non-linearity, we first transform the input features using techniques like polynomial features or basis functions. This means we create new variables (like squares or cubes of existing ones) to capture complex patterns. Then, Lasso applies its regularization, selecting only the most important transformed features. This approach allows Lasso to model non-linear relationships while keeping the model simple and avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3c431-ef0f-456c-ac4b-910372961833",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3134cef9-6911-4657-8c2b-dfba560ec8b5",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both techniques used to prevent overfitting in machine learning by adding a penalty to the model's complexity.\n",
    "\n",
    "Ridge Regression: It reduces large coefficients but doesn’t eliminate them completely. Think of it as spreading weight evenly among features rather than letting a few dominate.\n",
    "\n",
    "Lasso Regression: It can shrink some coefficients to zero, effectively selecting only the most important features. This makes it useful for feature selection.\n",
    "\n",
    "In simple terms, Ridge is like making all players contribute in a game, while Lasso picks only the best players and ignores the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5982a-a703-4d94-85c8-5d105eff1bf2",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d487e80-95c6-4c6c-a85f-f54a27bec448",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity (when input features are highly correlated). It does this by applying L1 regularization, which adds a penalty to the absolute values of the regression coefficients. This penalty forces some coefficients to shrink to exactly zero, effectively removing less important or redundant features.\n",
    "\n",
    "In simple terms, imagine choosing players for a cricket team. If two players have similar skills, you may drop one to avoid redundancy. Similarly, Lasso Regression eliminates less useful features, keeping only the most important ones. By doing this, it reduces overfitting and makes the model more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33afcf-d9cd-408c-bb29-e96bfb7b24cd",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaa82e-fe0f-45e3-a29f-fbc6abf14e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
